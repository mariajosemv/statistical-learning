\begin{frame}{Generative models}{Linear discriminant analysis for $p=1$}

\begin{itemize}
    \item For now, assume that $p=1$: we have only one predictor. \pause
    \item We want to obtain estimates for $f_k(x), \pi_k$ such as we can plug into (\ref{eq:bayes}) in order to estimate $p_k(x)$. \pause
    \item We'll assume that $f_k(x)$ is \textit{normal}, \pause

    \begin{equation}\label{eq:gaussian}
        f_k(x) = \frac{1}{\sqrt{ 2\pi } \sigma_k} \text{exp} \left(  - \frac{1}{2\sigma_k^2} (x - \mu_k)^2    \right)
    \end{equation} \pause

    \item Where $\mu_k$ and $\sigma_k^2$ are the mean and variance parameters for the $k$th class. \pause

    \item We'll assume constant variance so, $\sigma_1^2 = \cdots = \sigma_K^2 = \sigma^$. \pause

    \item Plugging (\ref{eq:gaussian}) into (\ref{eq:bayes}), results \pause

    \begin{equation}\label{eq:linear-prob}
        p_x(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi} \sigma}   \text{exp} \left(  - \frac{1}{2\sigma^2} (x - \mu_k)^2   \right)  }{  \sum_{l=1}^K  \pi_l \frac{1}{\sqrt{2\pi} \sigma}  \text{exp} \left(  - \frac{1}{2\sigma^2} (x - \mu_l)^2 \right) }
    \end{equation}
\end{itemize} 


\end{frame}


\begin{frame}{Generative models}{Linear discriminant analysis for $p=1$}

    \begin{equation*}
        p_x(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi} \sigma}   \text{exp} \left(  - \frac{1}{2\sigma^2} (x - \mu_k)^2    \right)  }{  \sum_{l=1}^K  \pi_l \frac{1}{\sqrt{2\pi} \sigma}   \text{exp} \left(  - \frac{1}{2\sigma^2} (x - \mu_l)^2    \right)       }
    \end{equation*}

\begin{itemize}

    \item Taking the log of (\ref{eq:linear-prob}) and rearranging the terms, \pause

    
    \begin{equation} \label{eq:delta-linear}
        \delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2} + \log{(\pi_k)}
    \end{equation} \pause

        \item To apply the Bayes classifier we still have to estimate the parameters $\pi_k$, $\mu_k$ and $\sigma^2$.  

\end{itemize}
    
\end{frame}


\begin{frame}{Generative models}{Linear discriminant analysis for $p=1$}

\begin{itemize}

    \item The following estimates are used: \pause

    \begin{align}
        \hat{\mu}_k &= \frac{1}{n_k} \sum_{i:y_i = k} x_i \\
        \hat{\sigma}^2 &= \frac{1}{n-K} \sum_{k=1}^K \sum_{i:y_i = k} (x_i - \hat{\mu}_k)^2.
    \end{align} \pause

    where $n$ is the total number of training observations, and $n_k$ is the number of training observations in the $k$th class. \pause

\item Sometimes we have knowledge of the class membership probabilities $\pi_1, \cdots, \pi_K $, which can be used directly. \pause

\item In the absence of any additional
information, LDA estimates $\pi_k$ using \pause

\begin{equation}
    \hat{\pi}_k = \frac{n_k}{n}
\end{equation}

\end{itemize}
    
\end{frame}

\begin{frame}{Generative models}{Linear discriminant analysis for $p=1$}

\begin{itemize}
    \item Now, equation (\ref{eq:delta-linear}) can be rewritten as \pause
    
    \begin{equation}\label{eq:delta-estimate}
        \hat{\delta}_k(x) = x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu_}k^2}{2 \hat{\sigma}^2} + \log{(\hat{\pi}_k)}
    \end{equation} \pause

    \item The Bayes decision boundary is the point for which $\hat{\delta}_1(x) = \hat{\delta}_2(x) = \cdots = \hat{\delta}_K(x) $ \pause

    \item The Bayes classifier involves assigning an observation $X = x$ to the class for which (\ref{eq:delta-estimate}) is \textcolor{blue}{largest}. \pause

    \item The word \textit{linear} in the classifier's name stems from the fact that the discriminant functions $\hat{\delta}_k(x)$ in (\ref{eq:delta-estimate}) are linear functions of $x$. \pause

\end{itemize}



\end{frame}